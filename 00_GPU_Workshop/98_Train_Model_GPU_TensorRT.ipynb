{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with GPU (and CPU*) and TensorRT\n",
    "CPU is still used to store variables that we are learning (`W` and `b`).  This allows the GPU to focus on compute vs. storage.\n",
    "\n",
    "TensorRT is used to optimize a trained TensorFlow Model (Graph + Variables/Weights) for Nvidia GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "import pylab\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset TensorFlow Graph\n",
    "Useful in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "  log_device_placement=True,\n",
    ")\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.4\n",
    "print(config)\n",
    "\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "print(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Model Version (current timestamp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "version = int(datetime.now().strftime(\"%s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Training and Test/Validation Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.rand(num_samples).astype(np.float32)\n",
    "print(x_train)\n",
    "\n",
    "noise = np.random.normal(scale=0.01, size=len(x_train))\n",
    "\n",
    "y_train = x_train * 0.1 + 0.3 + noise\n",
    "print(y_train)\n",
    "\n",
    "pylab.plot(x_train, y_train, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.rand(len(x_train)).astype(np.float32)\n",
    "print(x_test)\n",
    "\n",
    "noise = np.random.normal(scale=.01, size=len(x_train))\n",
    "\n",
    "y_test = x_test * 0.1 + 0.3 + noise\n",
    "print(y_test)\n",
    "\n",
    "pylab.plot(x_test, y_test, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    W = tf.get_variable(shape=[], name='weights')\n",
    "    print(W)\n",
    "\n",
    "    b = tf.get_variable(shape=[], name='bias')\n",
    "    print(b)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):            \n",
    "    x_observed = tf.placeholder(shape=[None], \n",
    "                                dtype=tf.float32, \n",
    "                                name='x_observed')\n",
    "    print(x_observed)\n",
    "\n",
    "    y_pred = W * x_observed + b\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.025\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    y_observed = tf.placeholder(shape=[None], dtype=tf.float32, name='y_observed')\n",
    "    print(y_observed)\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.square(y_pred - y_observed))\n",
    "    optimizer_op = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer_op.minimize(loss_op)  \n",
    "\n",
    "    print(\"Loss Scalar: \", loss_op)\n",
    "    print(\"Optimizer Op: \", optimizer_op)\n",
    "    print(\"Train Op: \", train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Initialize Variables (Weights and Bias)\n",
    "The goal is to learn more accurate Weights and Bias during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    print(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init_op)\n",
    "print(\"Initial random W: %f\" % sess.run(W))\n",
    "print(\"Initial random b: %f\" % sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Accuracy of Pre-Training, Initial Random Variables\n",
    "We want this to be close to 0, but it's relatively far away.  This is why we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, y):\n",
    "    return sess.run(loss_op, feed_dict={x_observed: x, y_observed: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Loss Summary Operations for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary_scalar_op = tf.summary.scalar('loss', loss_op)\n",
    "loss_summary_merge_all_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_writer = tf.summary.FileWriter('/root/pipelineai/models/optimize_me/linear/gpu/%s/train' % version, \n",
    "                                            graph=tf.get_default_graph())\n",
    "\n",
    "test_summary_writer = tf.summary.FileWriter('/root/pipelineai/models/optimize_me/linear/gpu/%s/test' % version,\n",
    "                                            graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    max_steps = 401\n",
    "    for step in range(max_steps):\n",
    "        if (step < max_steps - 1):\n",
    "            test_summary_log, _ = sess.run([loss_summary_merge_all_op, loss_op], feed_dict={x_observed: x_test, y_observed: y_test})\n",
    "            train_summary_log, _ = sess.run([loss_summary_merge_all_op, train_op], feed_dict={x_observed: x_train, y_observed: y_train})\n",
    "        else:  \n",
    "            test_summary_log, _ = sess.run([loss_summary_merge_all_op, loss_op], feed_dict={x_observed: x_test, y_observed: y_test})\n",
    "            train_summary_log, _ = sess.run([loss_summary_merge_all_op, train_op], feed_dict={x_observed: x_train, y_observed: y_train}, \n",
    "                                            options=tf.RunOptions(trace_level=tf.RunOptions.SOFTWARE_TRACE), \n",
    "                                            run_metadata=run_metadata)\n",
    "\n",
    "            trace = timeline.Timeline(step_stats=run_metadata.step_stats)    \n",
    "            with open('timeline-gpu.json', 'w') as trace_file:\n",
    "                trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(step, sess.run([W, b]))\n",
    "            train_summary_writer.add_summary(train_summary_log, step)\n",
    "            train_summary_writer.flush()\n",
    "            test_summary_writer.add_summary(test_summary_log, step)\n",
    "            test_summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.plot(x_train, y_train, '.', label=\"target\")\n",
    "pylab.plot(x_train, sess.run(y_pred, \n",
    "                             feed_dict={x_observed: x_train, \n",
    "                                        y_observed: y_train}), \n",
    "           \".\", \n",
    "           label=\"predicted\")\n",
    "pylab.legend()\n",
    "pylab.ylim(0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Loss Summaries in Tensorboard\n",
    "Navigate to the **`Scalars`** and **`Graphs`** tab in your PipelineAI App:\n",
    "\n",
    "https://[your-ip-address]/admin/tb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Graph For Optimization\n",
    "We will use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(name='/root/pipelineai/models/super_optimize_me/linear/gpu', exist_ok=True)\n",
    "optimize_me_parent_path = '/root/pipelineai/models/super_optimize_me/linear/gpu'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "unoptimized_model_graph_path = '%s/unoptimized_gpu.pb' % optimize_me_parent_path\n",
    "print(unoptimized_model_graph_path)\n",
    "\n",
    "tf.train.write_graph(sess.graph_def, \n",
    "                     '.', \n",
    "                     unoptimized_model_graph_path,\n",
    "                     as_text=False) \n",
    "\n",
    "\n",
    "model_checkpoint_path = '%s/model.ckpt' % optimize_me_parent_path\n",
    "saver.save(sess, \n",
    "           save_path=model_checkpoint_path)\n",
    "print(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimize_me_parent_path)\n",
    "os.listdir(optimize_me_parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import re\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "\n",
    "def convert_graph_to_dot(input_graph, output_dot, is_input_graph_binary):\n",
    "    graph = graph_pb2.GraphDef()\n",
    "    with open(input_graph, \"rb\") as fh:\n",
    "        if is_input_graph_binary:\n",
    "            graph.ParseFromString(fh.read())\n",
    "        else:\n",
    "            text_format.Merge(fh.read(), graph)\n",
    "    with open(output_dot, \"wt\") as fh:\n",
    "        print(\"digraph graphname {\", file=fh)\n",
    "        for node in graph.node:\n",
    "            output_name = node.name\n",
    "            print(\"  \\\"\" + output_name + \"\\\" [label=\\\"\" + node.op + \"\\\"];\", file=fh)\n",
    "            for input_full_name in node.input:\n",
    "                parts = input_full_name.split(\":\")\n",
    "                input_name = re.sub(r\"^\\^\", \"\", parts[0])\n",
    "                print(\"  \\\"\" + input_name + \"\\\" -> \\\"\" + output_name + \"\\\";\", file=fh)\n",
    "        print(\"}\", file=fh)\n",
    "        print(\"Created dot file '%s' for graph '%s'.\" % (output_dot, input_graph))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/pipelineai/models/optimize_me/linear/gpu/unoptimized_gpu.pb'\n",
    "output_dot='./unoptimized_gpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png ./unoptimized_gpu.dot \\\n",
    "    -o ./unoptimized_gpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('./unoptimized_gpu.png', width=1024, height=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate 4GB of our GPU for TensorRT\n",
    "workspace_size = 4000000000\n",
    "\n",
    "output_node_name = \n",
    "\n",
    "# Set input batch size to 128\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "trt_graph = trt.create_inference_graph(\n",
    "                input_graph_def=frozen_graph_def,\n",
    "                outputs=output_node_name,\n",
    "                max_batch_size=batch_size,\n",
    "                max_workspace_size_bytes=workspace_size,\n",
    "                precision_mode=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al tftrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tftrt/README\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tftrt/tftrt_sample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "#!/bin/env python -tt\n",
    "r\"\"\" TF-TensorRT integration sample script \"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "#from tensorflow.contrib import tensorrt as trt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.client import timeline\n",
    "import argparse, sys, itertools,datetime\n",
    "import json\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #selects a specific device\n",
    "\n",
    "def read_tensor_from_image_file(file_name, input_height=224, input_width=224,\n",
    "                                input_mean=0, input_std=255):\n",
    "  \"\"\" Read a jpg image file and return a tensor \"\"\"\n",
    "  input_name = \"file_reader\"\n",
    "  output_name = \"normalized\"\n",
    "  file_reader = tf.read_file(file_name, input_name)\n",
    "  image_reader = tf.image.decode_png(file_reader, channels = 3,\n",
    "                                       name='jpg_reader')\n",
    "  float_caster = tf.cast(image_reader, tf.float32)\n",
    "  dims_expander = tf.expand_dims(float_caster, 0);\n",
    "  resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "  normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "  sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50)))\n",
    "  result = sess.run([normalized,tf.transpose(normalized,perm=(0,3,1,2))])\n",
    "  del sess\n",
    "\n",
    "  return result\n",
    "\n",
    "def getSimpleGraphDef():\n",
    "  \"\"\"Create a simple graph and return its graph_def\"\"\"\n",
    "  if gfile.Exists(\"origgraph\"):\n",
    "    gfile.DeleteRecursively(\"origgraph\")\n",
    "  g = tf.Graph()\n",
    "  with g.as_default():\n",
    "    A = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3), name=\"input\")\n",
    "    e = tf.constant(\n",
    "        [[[[1., 0.5, 4., 6., 0.5, 1.], [1., 0.5, 1., 1., 0.5, 1.],[1.,1.,1.,1.,1.,1.]]]],\n",
    "        name=\"weights\",\n",
    "        dtype=tf.float32)\n",
    "    conv = tf.nn.conv2d(\n",
    "        input=A, filter=e, strides=[1, 1, 1, 1],dilations=[1,1,1,1], padding=\"SAME\", name=\"conv\")\n",
    "    b = tf.constant([4., 1.5, 2., 3., 5., 7.], name=\"bias\", dtype=tf.float32)\n",
    "    t = tf.nn.bias_add(conv, b, name=\"biasAdd\")\n",
    "    relu = tf.nn.relu(t, \"relu\")\n",
    "    idty = tf.identity(relu, \"ID\")\n",
    "    v = tf.nn.max_pool(\n",
    "        idty, [1, 2, 2, 1], [1, 2, 2, 1], \"VALID\", name=\"max_pool\")\n",
    "    out = tf.squeeze(v, name=\"resnet_v1_50/predictions/Reshape_1\")\n",
    "    writer = tf.summary.FileWriter(\"origgraph\", g)\n",
    "    writer.close()\n",
    "    \n",
    "  return g.as_graph_def()\n",
    "\n",
    "def updateGraphDef(fileName):\n",
    "  with gfile.FastGFile(fileName,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  tf.reset_default_graph()\n",
    "  g=tf.Graph()\n",
    "  with g.as_default():\n",
    "    tf.import_graph_def(graph_def,name=\"\")\n",
    "    with gfile.FastGFile(fileName,'wb') as f:\n",
    "      f.write(g.as_graph_def().SerializeToString())\n",
    "  \n",
    "def getResnet50():\n",
    "  with gfile.FastGFile(\"resnetV150_frozen.pb\",'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def\n",
    "\n",
    "def printStats(graphName,timings,batch_size):\n",
    "  if timings is None:\n",
    "    return\n",
    "  times=np.array(timings)\n",
    "  speeds=batch_size / times\n",
    "  avgTime=np.mean(timings)\n",
    "  avgSpeed=batch_size/avgTime\n",
    "  stdTime=np.std(timings)\n",
    "  stdSpeed=np.std(speeds)\n",
    "  print(\"images/s : %.1f +/- %.1f, s/batch: %.5f +/- %.5f\"%(avgSpeed,stdSpeed,avgTime,stdTime))\n",
    "  print(\"RES, %s, %s, %.2f, %.2f, %.5f, %.5f\"%(graphName,batch_size,avgSpeed,stdSpeed,avgTime,stdTime))\n",
    "\n",
    "def getFP32(batch_size=128,workspace_size=1<<30):\n",
    "  trt_graph = trt.create_inference_graph(getResnet50(), [ \"resnet_v1_50/predictions/Reshape_1\"],\n",
    "                                         max_batch_size=batch_size,\n",
    "                                         max_workspace_size_bytes=workspace_size,\n",
    "                                         precision_mode=\"FP32\")  # Get optimized graph\n",
    "  with gfile.FastGFile(\"resnetV150_TRTFP32.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph\n",
    "\n",
    "def getFP16(batch_size=128,workspace_size=1<<30):\n",
    "  trt_graph = trt.create_inference_graph(getResnet50(), [ \"resnet_v1_50/predictions/Reshape_1\"],\n",
    "                                         max_batch_size=batch_size,\n",
    "                                         max_workspace_size_bytes=workspace_size,\n",
    "                                         precision_mode=\"FP16\")  # Get optimized graph\n",
    "  with gfile.FastGFile(\"resnetV150_TRTFP16.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph\n",
    "\n",
    "def getINT8CalibGraph(batch_size=128,workspace_size=1<<30):\n",
    "  trt_graph = trt.create_inference_graph(getResnet50(), [ \"resnet_v1_50/predictions/Reshape_1\"],\n",
    "                                         max_batch_size=batch_size,\n",
    "                                         max_workspace_size_bytes=workspace_size,\n",
    "                                         precision_mode=\"INT8\")  # calibration\n",
    "  with gfile.FastGFile(\"resnetV150_TRTINT8Calib.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph\n",
    "\n",
    "def getINT8InferenceGraph(calibGraph):\n",
    "  trt_graph=trt.calib_graph_to_infer_graph(calibGraph)\n",
    "  with gfile.FastGFile(\"resnetV150_TRTINT8.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph\n",
    "\n",
    "def timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):\n",
    "  tf.logging.info(\"Starting execution\")\n",
    "  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.50)\n",
    "  tf.reset_default_graph()\n",
    "  g = tf.Graph()\n",
    "  if dummy_input is None:\n",
    "    dummy_input = np.random.random_sample((batch_size,224,224,3))\n",
    "  outlist=[]\n",
    "  with g.as_default():\n",
    "    inc=tf.constant(dummy_input, dtype=tf.float32)\n",
    "    dataset=tf.data.Dataset.from_tensors(inc)\n",
    "    dataset=dataset.repeat()\n",
    "    iterator=dataset.make_one_shot_iterator()\n",
    "    next_element=iterator.get_next()\n",
    "    out = tf.import_graph_def(\n",
    "      graph_def=gdef,\n",
    "      input_map={\"input\":next_element},\n",
    "      return_elements=[ \"resnet_v1_50/predictions/Reshape_1\"]\n",
    "    )\n",
    "    out = out[0].outputs[0]\n",
    "    outlist.append(out)\n",
    "    \n",
    "  timings=[]\n",
    "  \n",
    "  with tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    tf.logging.info(\"Starting Warmup cycle\")\n",
    "    def mergeTraceStr(mdarr):\n",
    "      tl=timeline.Timeline(mdarr[0][0].step_stats)\n",
    "      ctf=tl.generate_chrome_trace_format()\n",
    "      Gtf=json.loads(ctf)\n",
    "      deltat=mdarr[0][1][1]\n",
    "      for md in mdarr[1:]:\n",
    "        tl=timeline.Timeline(md[0].step_stats)\n",
    "        ctf=tl.generate_chrome_trace_format()\n",
    "        tmp=json.loads(ctf)\n",
    "        deltat=0\n",
    "        Gtf[\"traceEvents\"].extend(tmp[\"traceEvents\"])\n",
    "        deltat=md[1][1]\n",
    "        \n",
    "      return json.dumps(Gtf,indent=2)\n",
    "    rmArr=[[tf.RunMetadata(),0] for x in range(20)]\n",
    "    if timelineName:\n",
    "      if gfile.Exists(timelineName):\n",
    "        gfile.Remove(timelineName)\n",
    "      ttot=int(0)\n",
    "      tend=time.time()\n",
    "      for i in range(20):\n",
    "        tstart=time.time()\n",
    "        valt = sess.run(outlist,options=run_options,run_metadata=rmArr[i][0])\n",
    "        tend=time.time()\n",
    "        rmArr[i][1]=(int(tstart*1.e6),int(tend*1.e6))\n",
    "      with gfile.FastGFile(timelineName,\"a\") as tlf:\n",
    "        tlf.write(mergeTraceStr(rmArr))\n",
    "    else:\n",
    "      for i in range(20):\n",
    "        valt = sess.run(outlist)\n",
    "    tf.logging.info(\"Warmup done. Starting real timing\")\n",
    "    num_iters=50\n",
    "    for i in range(num_loops):\n",
    "      tstart=time.time()\n",
    "      for k in range(num_iters):\n",
    "        val = sess.run(outlist)\n",
    "      timings.append((time.time()-tstart)/float(num_iters))\n",
    "      print(\"iter \",i,\" \",timings[-1])\n",
    "    comp=sess.run(tf.reduce_all(tf.equal(val[0],valt[0])))\n",
    "    print(\"Comparison=\",comp)\n",
    "    sess.close()\n",
    "    tf.logging.info(\"Timing loop done!\")\n",
    "    return timings,comp,val[0],None\n",
    "\n",
    "def score(nat,trt,topN=5):\n",
    "  ind=np.argsort(nat)[:,-topN:]\n",
    "  tind=np.argsort(trt)[:,-topN:]\n",
    "  return np.array_equal(ind,tind),howClose(nat,trt,topN)\n",
    "\n",
    "def topX(arr,X):\n",
    "  ind=np.argsort(arr)[:,-X:][:,::-1]\n",
    "  return arr[np.arange(np.shape(arr)[0])[:,np.newaxis],ind],ind\n",
    "\n",
    "def howClose(arr1,arr2,X):\n",
    "  val1,ind1=topX(arr1,X)\n",
    "  val2,ind2=topX(arr2,X)\n",
    "  ssum=0.\n",
    "  for i in range(X):\n",
    "    in1=ind1[0]\n",
    "    in2=ind2[0]\n",
    "    if(in1[i]==in2[i]):\n",
    "      ssum+=1\n",
    "    else:\n",
    "      pos=np.where(in2==in1[i])\n",
    "      pos=pos[0]\n",
    "      if pos.shape[0]:\n",
    "        if np.abs(pos[0]-i)<2:\n",
    "          ssum+=0.5\n",
    "  return ssum/X\n",
    "\n",
    "def getLabels(labels,ids):\n",
    "  return [labels[str(x+1)] for x in ids]\n",
    "\n",
    "if \"__main__\" in __name__:\n",
    "  P=argparse.ArgumentParser(prog=\"test\")\n",
    "  P.add_argument('--FP32',action='store_true')\n",
    "  P.add_argument('--FP16',action='store_true')\n",
    "  P.add_argument('--INT8',action='store_true')\n",
    "  P.add_argument('--native',action='store_true')\n",
    "  P.add_argument('--num_loops',type=int,default=20)\n",
    "  P.add_argument('--topN',type=int,default=10)\n",
    "  P.add_argument('--batch_size',type=int,default=128)\n",
    "  P.add_argument('--dump_diff',action='store_true')\n",
    "  P.add_argument('--with_timeline',action='store_true')\n",
    "  P.add_argument('--workspace_size',type=int,default=1<<10,help=\"workspace size in MB\")\n",
    "  P.add_argument('--update_graphdef',action='store_true')\n",
    "  \n",
    "  f,unparsed=P.parse_known_args()\n",
    "  print(f)\n",
    "  valnative=None\n",
    "  valfp32=None\n",
    "  valfp16=None\n",
    "  valint8=None\n",
    "  res=[None,None,None,None]\n",
    "  print(\"Starting at\",datetime.datetime.now())\n",
    "  if f.update_graphdef:\n",
    "    updateGraphDef(\"resnetV150_frozen.pb\")\n",
    "  dummy_input = np.random.random_sample((f.batch_size,224,224,3))\n",
    "  with open(\"labellist.json\",\"r\") as lf:\n",
    "    labels=json.load(lf)\n",
    "  imageName=\"grace_hopper.jpg\"\n",
    "  t = read_tensor_from_image_file(imageName,\n",
    "                                  input_height=224,\n",
    "                                  input_width=224,\n",
    "                                  input_mean=0,\n",
    "                                  input_std=1.0)\n",
    "  tshape=list(t[0].shape)\n",
    "  tshape[0]=f.batch_size\n",
    "  tnhwcbatch=np.tile(t[0],(f.batch_size,1,1,1))\n",
    "  dummy_input=tnhwcbatch\n",
    "  wsize=f.workspace_size<<20\n",
    "  timelineName=None\n",
    "  if f.native:\n",
    "    if f.with_timeline: timelineName=\"NativeTimeline.json\"\n",
    "    timings,comp,valnative,mdstats=timeGraph(getResnet50(),f.batch_size,\n",
    "                                     f.num_loops,dummy_input,timelineName)\n",
    "    printStats(\"Native\",timings,f.batch_size)\n",
    "    printStats(\"NativeRS\",mdstats,f.batch_size)\n",
    "  if f.FP32:\n",
    "    if f.with_timeline: timelineName=\"FP32Timeline.json\"\n",
    "    timings,comp,valfp32,mdstats=timeGraph(getFP32(f.batch_size,wsize),f.batch_size,f.num_loops,\n",
    "                                   dummy_input,timelineName)\n",
    "    printStats(\"TRT-FP32\",timings,f.batch_size)\n",
    "    printStats(\"TRT-FP32RS\",mdstats,f.batch_size)\n",
    "  if f.FP16:\n",
    "    k=0\n",
    "    if f.with_timeline: timelineName=\"FP16Timeline.json\"\n",
    "    timings,comp,valfp16,mdstats=timeGraph(getFP16(f.batch_size,wsize),f.batch_size,\n",
    "                                   f.num_loops,dummy_input,timelineName)\n",
    "    printStats(\"TRT-FP16\",timings,f.batch_size)\n",
    "    printStats(\"TRT-FP16RS\",mdstats,f.batch_size)\n",
    "  if f.INT8:\n",
    "    calibGraph=getINT8CalibGraph(f.batch_size,wsize)\n",
    "    print(\"Running Calibration\")\n",
    "    timings,comp,_,mdstats=timeGraph(calibGraph,f.batch_size,1,dummy_input)\n",
    "    print(\"Creating inference graph\")\n",
    "    int8Graph=getINT8InferenceGraph(calibGraph)\n",
    "    del calibGraph\n",
    "    if f.with_timeline: timelineName=\"INT8Timeline.json\"\n",
    "    timings,comp,valint8,mdstats=timeGraph(int8Graph,f.batch_size,\n",
    "                                   f.num_loops,dummy_input,timelineName)\n",
    "    printStats(\"TRT-INT8\",timings,f.batch_size)\n",
    "    printStats(\"TRT-INT8RS\",mdstats,f.batch_size)\n",
    "  vals=[valnative,valfp32,valfp16,valint8]\n",
    "  enabled=[(f.native,\"native\",valnative),\n",
    "           (f.FP32,\"FP32\",valfp32),\n",
    "           (f.FP16,\"FP16\",valfp16),\n",
    "           (f.INT8,\"INT8\",valint8)]\n",
    "  print(\"Done timing\",datetime.datetime.now())\n",
    "  for i in enabled:\n",
    "    if i[0]:\n",
    "      print(i[1],getLabels(labels,topX(i[2],f.topN)[1][0]))\n",
    "    \n",
    "#  sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
