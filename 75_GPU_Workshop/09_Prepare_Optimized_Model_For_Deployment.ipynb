{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Optimized Model for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT:  You Must STOP All Kernels and Terminal Session\n",
    "The GPU is wedged at this point.  We need to set it free!!\n",
    "\n",
    "![Shutdown All Kernels and Terminals](http://pipeline.io/img/shutdown-all-kernels-and-terminals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze Fully Optimized Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "\n",
    "optimize_me_parent_path = '/root/models/optimize_me/linear/cpu'\n",
    "\n",
    "fully_optimized_model_graph_path = '%s/fully_optimized_cpu.pb' % optimize_me_parent_path\n",
    "fully_optimized_frozen_model_graph_path = '%s/fully_optimized_frozen_cpu.pb' % optimize_me_parent_path\n",
    "\n",
    "model_checkpoint_path = '%s/model.ckpt' % optimize_me_parent_path\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph=fully_optimized_model_graph_path, \n",
    "                          input_saver=\"\",\n",
    "                          input_binary=True, \n",
    "                          input_checkpoint='/root/models/optimize_me/linear/cpu/model.ckpt',\n",
    "                          output_node_names=\"add\",\n",
    "                          restore_op_name=\"save/restore_all\", \n",
    "                          filename_tensor_name=\"save/Const:0\",\n",
    "                          output_graph=fully_optimized_frozen_model_graph_path, \n",
    "                          clear_devices=True, \n",
    "                          initializer_nodes=\"\")\n",
    "print(fully_optimized_frozen_model_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/linear/cpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/linear/cpu/fully_optimized_frozen_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import re\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "\n",
    "def convert_graph_to_dot(input_graph, output_dot, is_input_graph_binary):\n",
    "    graph = graph_pb2.GraphDef()\n",
    "    with open(input_graph, \"rb\") as fh:\n",
    "        if is_input_graph_binary:\n",
    "            graph.ParseFromString(fh.read())\n",
    "        else:\n",
    "            text_format.Merge(fh.read(), graph)\n",
    "    with open(output_dot, \"wt\") as fh:\n",
    "        print(\"digraph graphname {\", file=fh)\n",
    "        for node in graph.node:\n",
    "            output_name = node.name\n",
    "            print(\"  \\\"\" + output_name + \"\\\" [label=\\\"\" + node.op + \"\\\"];\", file=fh)\n",
    "            for input_full_name in node.input:\n",
    "                parts = input_full_name.split(\":\")\n",
    "                input_name = re.sub(r\"^\\^\", \"\", parts[0])\n",
    "                print(\"  \\\"\" + input_name + \"\\\" -> \\\"\" + output_name + \"\\\";\", file=fh)\n",
    "        print(\"}\", file=fh)\n",
    "        print(\"Created dot file '%s' for graph '%s'.\" % (output_dot, input_graph))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/linear/cpu/fully_optimized_frozen_cpu.pb'\n",
    "output_dot='./fully_optimized_frozen_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png ./fully_optimized_frozen_cpu.dot \\\n",
    "    -o ./fully_optimized_frozen_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('./fully_optimized_frozen_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Standalone Benchmarks\n",
    "Note:  These benchmarks are running against the standalone models on disk.  We will benchmark the models running within TensorFlow Serving soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/linear/cpu/fully_optimized_frozen_cpu.pb \\\n",
    "    --input_layer=weights,bias,x_observed \\\n",
    "    --input_layer_type=float,float,float \\\n",
    "    --input_layer_shape=:: \\\n",
    "    --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model for Deployment and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Default Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Version Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "version = int(datetime.now().strftime(\"%s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Optimized, Frozen Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import inspect_checkpoint\n",
    "\n",
    "inspect_checkpoint.print_tensors_in_checkpoint_file(file_name=\"/root/models/optimize_me/linear/cpu/model.ckpt\",\n",
    "                                                    tensor_name=\"\",\n",
    "                                                    all_tensors=True,\n",
    "                                                    all_tensor_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph('/root/models/optimize_me/linear/cpu/model.ckpt.meta')\n",
    "saver.restore(sess, '/root/models/optimize_me/linear/cpu/model.ckpt')\n",
    "\n",
    "optimize_me_parent_path = '/root/models/optimize_me/linear/cpu'\n",
    "fully_optimized_frozen_model_graph_path = '%s/fully_optimized_frozen_cpu.pb' % optimize_me_parent_path\n",
    "print(fully_optimized_frozen_model_graph_path)\n",
    "\n",
    "with tf.gfile.GFile(fully_optimized_frozen_model_graph_path, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "tf.import_graph_def(\n",
    "    graph_def, \n",
    "    input_map=None, \n",
    "    return_elements=None, \n",
    "    name=\"\", \n",
    "    op_dict=None, \n",
    "    producer_op_list=None\n",
    ")\n",
    "\n",
    "print(\"weights = \", sess.run(\"weights:0\"))\n",
    "print(\"bias = \", sess.run(\"bias:0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `SignatureDef` Asset for TensorFlow Serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "x_observed = graph.get_tensor_by_name('x_observed:0')\n",
    "y_pred = graph.get_tensor_by_name('add:0')\n",
    "\n",
    "inputs_map = {'inputs': x_observed}\n",
    "outputs_map = {'outputs': y_pred}\n",
    "\n",
    "predict_signature = signature_def_utils.predict_signature_def(\n",
    "                inputs = inputs_map, \n",
    "                outputs = outputs_map)\n",
    "print(predict_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model with Assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "fully_optimized_saved_model_path = '/root/models/linear_fully_optimized/cpu/%s' % version\n",
    "print(fully_optimized_saved_model_path)\n",
    "\n",
    "builder = saved_model_builder.SavedModelBuilder(fully_optimized_saved_model_path)\n",
    "builder.add_meta_graph_and_variables(sess, \n",
    "                                     [tag_constants.SERVING],\n",
    "                                     signature_def_map={'predict':predict_signature,                                     \n",
    "signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:predict_signature}, \n",
    "                                     clear_devices=True,\n",
    ")\n",
    "\n",
    "builder.save(as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(fully_optimized_saved_model_path)\n",
    "os.listdir(fully_optimized_saved_model_path)\n",
    "os.listdir('%s/variables' % fully_optimized_saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect with [Saved Model CLI](https://www.tensorflow.org/programmers_guide/saved_model_cli)\n",
    "Note:  This takes a minute or two for some reason.  Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.run([\"saved_model_cli\", \"show\", \\\n",
    "                \"--dir\", fully_optimized_saved_model_path, \"--all\"], \\\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE)\n",
    "\n",
    "print(output.stdout.decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
