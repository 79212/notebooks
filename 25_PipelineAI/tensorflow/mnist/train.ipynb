{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from datetime import datetime\n",
    "_version = int(datetime.now().strftime(\"%s\"))\n",
    "\n",
    "def init_flags():\n",
    "    global FLAGS\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--rundir\", default=\"./runs\")\n",
    "    parser.add_argument(\"--datadir\", default=\"./runs/data\")\n",
    "    parser.add_argument(\"--servingdir\", default=\"./versions\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1000)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--prepare\", dest='just_data', action=\"store_true\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.025)\n",
    "    FLAGS, _ = parser.parse_known_args()\n",
    "\n",
    "def init_data():\n",
    "    global mnist\n",
    "    mnist = input_data.read_data_sets(FLAGS.datadir, one_hot=True)\n",
    "\n",
    "def init_train():\n",
    "    init_model()\n",
    "    init_train_op()\n",
    "    init_eval_op()\n",
    "    init_summaries()\n",
    "    init_collections()\n",
    "    init_session()\n",
    "\n",
    "def init_model():\n",
    "    global x, y, W, b\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "def init_train_op():\n",
    "    global y_, loss, train_op\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    loss = tf.reduce_mean(\n",
    "             -tf.reduce_sum(\n",
    "               y_ * tf.log(y),\n",
    "               reduction_indices=[1]))\n",
    "    train_op = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(loss)\n",
    "\n",
    "def init_eval_op():\n",
    "    global accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def init_summaries():\n",
    "    init_inputs_summary()\n",
    "    init_variable_summaries(W, \"weights\")\n",
    "    init_variable_summaries(b, \"biases\")\n",
    "    init_op_summaries()\n",
    "    init_summary_writers()\n",
    "\n",
    "def init_inputs_summary():\n",
    "    tf.summary.image(\"inputs\", tf.reshape(x, [-1, 28, 28, 1]), 10)\n",
    "\n",
    "def init_variable_summaries(var, name):\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "\n",
    "def init_op_summaries():\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "def init_summary_writers():\n",
    "    global summaries, train_writer, validation_writer\n",
    "    summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\n",
    "        FLAGS.rundir + \"/train\",\n",
    "        tf.get_default_graph())\n",
    "    validation_writer = tf.summary.FileWriter(\n",
    "        FLAGS.rundir + \"/validation\")\n",
    "\n",
    "def init_collections():\n",
    "    tf.add_to_collection(\"inputs\", json.dumps({\"image\": x.name}))\n",
    "    tf.add_to_collection(\"outputs\", json.dumps({\"prediction\": y.name}))\n",
    "    tf.add_to_collection(\"x\", x.name)\n",
    "    tf.add_to_collection(\"y_\", y_.name)\n",
    "    tf.add_to_collection(\"accuracy\", accuracy.name)\n",
    "\n",
    "def init_session():\n",
    "    global sess\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def train():\n",
    "    steps = (mnist.train.num_examples // FLAGS.batch_size) * FLAGS.epochs\n",
    "    for step in range(steps + 1):\n",
    "        images, labels = mnist.train.next_batch(FLAGS.batch_size)\n",
    "        batch = {x: images, y_: labels}\n",
    "        sess.run(train_op, batch)\n",
    "        maybe_log_accuracy(step, batch)\n",
    "    save_model()\n",
    "\n",
    "def maybe_log_accuracy(step, last_training_batch):\n",
    "    if step % 20 == 0:\n",
    "        evaluate(step, last_training_batch, train_writer, \"training\")\n",
    "        validation_data = {\n",
    "            x: mnist.validation.images,\n",
    "            y_: mnist.validation.labels\n",
    "        }\n",
    "        evaluate(step, validation_data, validation_writer, \"validation\")\n",
    "\n",
    "def evaluate(step, data, writer, name):\n",
    "    accuracy_val, summary = sess.run([accuracy, summaries], data)\n",
    "    writer.add_summary(summary, step)\n",
    "    print(\"Step %i: %s=%f\" % (step, name, accuracy_val))\n",
    "\n",
    "def maybe_save_model(step):\n",
    "    epoch_step = mnist.train.num_examples / FLAGS.batch_size\n",
    "    if step != 0 and step % epoch_step == 0:\n",
    "        save_model()\n",
    "\n",
    "def save_model():\n",
    "    print(\"Saving trained model\")\n",
    "    tf.gfile.MakeDirs(FLAGS.rundir + \"/model\")\n",
    "    exported_model_path = FLAGS.rundir + \"/model/export\"\n",
    "    tf.train.Saver().save(sess, exported_model_path)\n",
    "\n",
    "    from tensorflow.python.saved_model import utils\n",
    "    from tensorflow.python.saved_model import signature_constants\n",
    "    from tensorflow.python.saved_model import signature_def_utils\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    inputs_map = {'inputs': x}\n",
    "    outputs_map = {'outputs': y}\n",
    "\n",
    "    prediction_signature = signature_def_utils.predict_signature_def(inputs=inputs_map,\n",
    "                                                                     outputs=outputs_map)\n",
    "\n",
    "    from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "    saved_model_path = '%s/%s' % (FLAGS.servingdir, _version)\n",
    "    print(saved_model_path)\n",
    "\n",
    "    builder = saved_model_builder.SavedModelBuilder(saved_model_path)\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tag_constants.SERVING],\n",
    "                                         signature_def_map={'predict':prediction_signature,\n",
    "    signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:prediction_signature},\n",
    "                                         clear_devices=True,\n",
    "    )\n",
    "\n",
    "    builder.save(as_text=False)\n",
    "    print(\"\")\n",
    "\n",
    "#    served_model_path = '%s/%s' % (FLAGS.servingdir, _version)\n",
    "\n",
    "    print(\"Training complete.  tf.train.Saver exported to '%s'.\\nSavedModelBuilder saved to '%s'.\" % (exported_model_path, saved_model_path))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def init_test():\n",
    "    init_session()\n",
    "    init_exported_collections()\n",
    "\n",
    "\n",
    "def init_exported_collections():\n",
    "    global x, y_, accuracy\n",
    "    saver = tf.train.import_meta_graph(FLAGS.rundir + \"/model/export.meta\")\n",
    "    saver.restore(sess, FLAGS.rundir + \"/model/export\")\n",
    "    x = sess.graph.get_tensor_by_name(tf.get_collection(\"x\")[0])\n",
    "    y_ = sess.graph.get_tensor_by_name(tf.get_collection(\"y_\")[0])\n",
    "    accuracy = sess.graph.get_tensor_by_name(tf.get_collection(\"accuracy\")[0])\n",
    "\n",
    "def test():\n",
    "    data = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "    test_accuracy = sess.run(accuracy, data)\n",
    "    print(\"Test accuracy=%f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./runs/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./runs/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./runs/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./runs/data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: training=0.490000\n",
      "Step 0: validation=0.460000\n",
      "Step 20: training=0.738000\n",
      "Step 20: validation=0.767400\n",
      "Step 40: training=0.783000\n",
      "Step 40: validation=0.789000\n",
      "Step 60: training=0.774000\n",
      "Step 60: validation=0.796000\n",
      "Step 80: training=0.795000\n",
      "Step 80: validation=0.808000\n",
      "Step 100: training=0.799000\n",
      "Step 100: validation=0.821600\n",
      "Step 120: training=0.834000\n",
      "Step 120: validation=0.832400\n",
      "Step 140: training=0.799000\n",
      "Step 140: validation=0.836800\n",
      "Step 160: training=0.831000\n",
      "Step 160: validation=0.839000\n",
      "Step 180: training=0.851000\n",
      "Step 180: validation=0.845400\n",
      "Step 200: training=0.842000\n",
      "Step 200: validation=0.848600\n",
      "Step 220: training=0.844000\n",
      "Step 220: validation=0.851400\n",
      "Step 240: training=0.857000\n",
      "Step 240: validation=0.853600\n",
      "Step 260: training=0.841000\n",
      "Step 260: validation=0.855400\n",
      "Step 280: training=0.858000\n",
      "Step 280: validation=0.857200\n",
      "Step 300: training=0.871000\n",
      "Step 300: validation=0.860000\n",
      "Step 320: training=0.855000\n",
      "Step 320: validation=0.862000\n",
      "Step 340: training=0.859000\n",
      "Step 340: validation=0.864400\n",
      "Step 360: training=0.860000\n",
      "Step 360: validation=0.865400\n",
      "Step 380: training=0.861000\n",
      "Step 380: validation=0.866400\n",
      "Step 400: training=0.858000\n",
      "Step 400: validation=0.866800\n",
      "Step 420: training=0.879000\n",
      "Step 420: validation=0.869600\n",
      "Step 440: training=0.880000\n",
      "Step 440: validation=0.870400\n",
      "Step 460: training=0.862000\n",
      "Step 460: validation=0.872400\n",
      "Step 480: training=0.875000\n",
      "Step 480: validation=0.872800\n",
      "Step 500: training=0.883000\n",
      "Step 500: validation=0.874600\n",
      "Step 520: training=0.869000\n",
      "Step 520: validation=0.874400\n",
      "Step 540: training=0.862000\n",
      "Step 540: validation=0.875800\n",
      "Saving trained model\n",
      "./versions/1508261106\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./versions/1508261106/saved_model.pb'\n",
      "\n",
      "Training complete.  tf.train.Saver exported to './runs/model/export'.\n",
      "SavedModelBuilder saved to './versions/1508261106'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_flags()\n",
    "init_data()\n",
    "if FLAGS.just_data:\n",
    "    pass\n",
    "elif FLAGS.test:\n",
    "    init_test()\n",
    "    test()\n",
    "else:\n",
    "    init_train()\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
