{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from datetime import datetime\n",
    "_version = int(datetime.now().strftime(\"%s\"))\n",
    "\n",
    "def init_flags():\n",
    "    global FLAGS\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--rundir\", default=\"./runs\")\n",
    "    parser.add_argument(\"--datadir\", default=\"./runs/data\")\n",
    "    parser.add_argument(\"--servingdir\", default=\"./versions\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1000)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--prepare\", dest='just_data', action=\"store_true\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.025)\n",
    "    FLAGS, _ = parser.parse_known_args()\n",
    "\n",
    "def init_data():\n",
    "    global mnist\n",
    "    mnist = input_data.read_data_sets(FLAGS.datadir, one_hot=True)\n",
    "\n",
    "def init_train():\n",
    "    init_model()\n",
    "    init_train_op()\n",
    "    init_eval_op()\n",
    "    init_summaries()\n",
    "    init_collections()\n",
    "    init_session()\n",
    "\n",
    "def init_model():\n",
    "    global x, y, W, b\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "def init_train_op():\n",
    "    global y_, loss, train_op\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    loss = tf.reduce_mean(\n",
    "             -tf.reduce_sum(\n",
    "               y_ * tf.log(y),\n",
    "               reduction_indices=[1]))\n",
    "    train_op = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(loss)\n",
    "\n",
    "def init_eval_op():\n",
    "    global accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def init_summaries():\n",
    "    init_inputs_summary()\n",
    "    init_variable_summaries(W, \"weights\")\n",
    "    init_variable_summaries(b, \"biases\")\n",
    "    init_op_summaries()\n",
    "    init_summary_writers()\n",
    "\n",
    "def init_inputs_summary():\n",
    "    tf.summary.image(\"inputs\", tf.reshape(x, [-1, 28, 28, 1]), 10)\n",
    "\n",
    "def init_variable_summaries(var, name):\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "\n",
    "def init_op_summaries():\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "def init_summary_writers():\n",
    "    global summaries, train_writer, validation_writer\n",
    "    summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\n",
    "        FLAGS.rundir + \"/train\",\n",
    "        tf.get_default_graph())\n",
    "    validation_writer = tf.summary.FileWriter(\n",
    "        FLAGS.rundir + \"/validation\")\n",
    "\n",
    "def init_collections():\n",
    "    tf.add_to_collection(\"inputs\", json.dumps({\"image\": x.name}))\n",
    "    tf.add_to_collection(\"outputs\", json.dumps({\"prediction\": y.name}))\n",
    "    tf.add_to_collection(\"x\", x.name)\n",
    "    tf.add_to_collection(\"y_\", y_.name)\n",
    "    tf.add_to_collection(\"accuracy\", accuracy.name)\n",
    "\n",
    "def init_session():\n",
    "    global sess\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def train():\n",
    "    steps = (mnist.train.num_examples // FLAGS.batch_size) * FLAGS.epochs\n",
    "    for step in range(steps + 1):\n",
    "        images, labels = mnist.train.next_batch(FLAGS.batch_size)\n",
    "        batch = {x: images, y_: labels}\n",
    "        sess.run(train_op, batch)\n",
    "        maybe_log_accuracy(step, batch)\n",
    "    save_model()\n",
    "\n",
    "def maybe_log_accuracy(step, last_training_batch):\n",
    "    if step % 20 == 0:\n",
    "        evaluate(step, last_training_batch, train_writer, \"training\")\n",
    "        validation_data = {\n",
    "            x: mnist.validation.images,\n",
    "            y_: mnist.validation.labels\n",
    "        }\n",
    "        evaluate(step, validation_data, validation_writer, \"validation\")\n",
    "\n",
    "def evaluate(step, data, writer, name):\n",
    "    accuracy_val, summary = sess.run([accuracy, summaries], data)\n",
    "    writer.add_summary(summary, step)\n",
    "    print(\"Step %i: %s=%f\" % (step, name, accuracy_val))\n",
    "\n",
    "def maybe_save_model(step):\n",
    "    epoch_step = mnist.train.num_examples / FLAGS.batch_size\n",
    "    if step != 0 and step % epoch_step == 0:\n",
    "        save_model()\n",
    "\n",
    "def save_model():\n",
    "    print(\"Saving trained model\")\n",
    "    tf.gfile.MakeDirs(FLAGS.rundir + \"/model\")\n",
    "    exported_model_path = FLAGS.rundir + \"/model/export\"\n",
    "    tf.train.Saver().save(sess, exported_model_path)\n",
    "\n",
    "    from tensorflow.python.saved_model import utils\n",
    "    from tensorflow.python.saved_model import signature_constants\n",
    "    from tensorflow.python.saved_model import signature_def_utils\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    inputs_map = {'inputs': x}\n",
    "    outputs_map = {'outputs': y}\n",
    "\n",
    "    prediction_signature = signature_def_utils.predict_signature_def(inputs=inputs_map,\n",
    "                                                                     outputs=outputs_map)\n",
    "\n",
    "    from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "    saved_model_path = '%s/%s' % (FLAGS.servingdir, _version)\n",
    "    print(saved_model_path)\n",
    "\n",
    "    builder = saved_model_builder.SavedModelBuilder(saved_model_path)\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tag_constants.SERVING],\n",
    "                                         signature_def_map={'predict':prediction_signature,\n",
    "    signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:prediction_signature},\n",
    "                                         clear_devices=True,\n",
    "    )\n",
    "\n",
    "    builder.save(as_text=False)\n",
    "    print(\"\")\n",
    "\n",
    "#    served_model_path = '%s/%s' % (FLAGS.servingdir, _version)\n",
    "\n",
    "    print(\"Training complete.  tf.train.Saver exported to '%s'.\\nSavedModelBuilder saved to '%s'.\" % (exported_model_path, saved_model_path))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def init_test():\n",
    "    init_session()\n",
    "    init_exported_collections()\n",
    "\n",
    "\n",
    "def init_exported_collections():\n",
    "    global x, y_, accuracy\n",
    "    saver = tf.train.import_meta_graph(FLAGS.rundir + \"/model/export.meta\")\n",
    "    saver.restore(sess, FLAGS.rundir + \"/model/export\")\n",
    "    x = sess.graph.get_tensor_by_name(tf.get_collection(\"x\")[0])\n",
    "    y_ = sess.graph.get_tensor_by_name(tf.get_collection(\"y_\")[0])\n",
    "    accuracy = sess.graph.get_tensor_by_name(tf.get_collection(\"accuracy\")[0])\n",
    "\n",
    "def test():\n",
    "    data = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "    test_accuracy = sess.run(accuracy, data)\n",
    "    print(\"Test accuracy=%f\" % test_accuracy)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    init_flags()\n",
    "#    init_data()\n",
    "#    if FLAGS.just_data:\n",
    "#        pass\n",
    "#    elif FLAGS.test:\n",
    "#        init_test()\n",
    "#        test()\n",
    "#    else:\n",
    "#        init_train()\n",
    "#        train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./runs/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./runs/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./runs/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./runs/data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: training=0.552000\n",
      "Step 0: validation=0.491200\n",
      "Step 20: training=0.749000\n",
      "Step 20: validation=0.744000\n",
      "Step 40: training=0.778000\n",
      "Step 40: validation=0.788800\n",
      "Step 60: training=0.801000\n",
      "Step 60: validation=0.798800\n",
      "Step 80: training=0.802000\n",
      "Step 80: validation=0.810200\n",
      "Step 100: training=0.794000\n",
      "Step 100: validation=0.820400\n",
      "Step 120: training=0.819000\n",
      "Step 120: validation=0.830400\n",
      "Step 140: training=0.828000\n",
      "Step 140: validation=0.836200\n",
      "Step 160: training=0.825000\n",
      "Step 160: validation=0.838800\n",
      "Step 180: training=0.833000\n",
      "Step 180: validation=0.846000\n",
      "Step 200: training=0.833000\n",
      "Step 200: validation=0.848400\n",
      "Step 220: training=0.846000\n",
      "Step 220: validation=0.851400\n",
      "Step 240: training=0.843000\n",
      "Step 240: validation=0.853600\n",
      "Step 260: training=0.857000\n",
      "Step 260: validation=0.854600\n",
      "Step 280: training=0.860000\n",
      "Step 280: validation=0.858000\n",
      "Step 300: training=0.833000\n",
      "Step 300: validation=0.859800\n",
      "Step 320: training=0.855000\n",
      "Step 320: validation=0.861800\n",
      "Step 340: training=0.860000\n",
      "Step 340: validation=0.863600\n",
      "Step 360: training=0.862000\n",
      "Step 360: validation=0.865400\n",
      "Step 380: training=0.855000\n",
      "Step 380: validation=0.866800\n",
      "Step 400: training=0.851000\n",
      "Step 400: validation=0.868400\n",
      "Step 420: training=0.847000\n",
      "Step 420: validation=0.868600\n",
      "Step 440: training=0.855000\n",
      "Step 440: validation=0.870600\n",
      "Step 460: training=0.877000\n",
      "Step 460: validation=0.872400\n",
      "Step 480: training=0.862000\n",
      "Step 480: validation=0.873000\n",
      "Step 500: training=0.875000\n",
      "Step 500: validation=0.874000\n",
      "Step 520: training=0.877000\n",
      "Step 520: validation=0.874600\n",
      "Step 540: training=0.852000\n",
      "Step 540: validation=0.875600\n",
      "Saving trained model\n",
      "./versions/1507628407\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./versions/1507628407/saved_model.pb'\n",
      "\n",
      "Training complete.  tf.train.Saver exported to './runs/model/export'.\n",
      "SavedModelBuilder saved to './versions/1507628407'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_flags()\n",
    "init_data()\n",
    "if FLAGS.just_data:\n",
    "    pass\n",
    "elif FLAGS.test:\n",
    "    init_test()\n",
    "    test()\n",
    "else:\n",
    "    init_train()\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: docker: not found\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cli-pipeline==1.3.0\r\n",
      "  Downloading cli_pipeline-1.3.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: tabulate==0.7.7 in /opt/conda/lib/python3.5/site-packages (from cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: pyyaml==3.12 in /opt/conda/lib/python3.5/site-packages (from cli-pipeline==1.3.0)\r\n",
      "Collecting kubernetes==3.0.0 (from cli-pipeline==1.3.0)\r\n",
      "  Using cached kubernetes-3.0.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: requests==2.18.1 in /opt/conda/lib/python3.5/site-packages (from cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: fire==0.1.1 in /opt/conda/lib/python3.5/site-packages (from cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: futures==3.1.1 in /opt/conda/lib/python3.5/site-packages (from cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: jinja2==2.9.6 in /opt/conda/lib/python3.5/site-packages (from cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: ipaddress>=1.0.17 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: urllib3!=1.21,>=1.19.1 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: websocket-client<=0.40.0,>=0.32.0 in /opt/conda/lib/python3.5/site-packages (from kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: idna<2.6,>=2.5 in /opt/conda/lib/python3.5/site-packages (from requests==2.18.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.5/site-packages (from requests==2.18.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: ipython<6.0 in /opt/conda/lib/python3.5/site-packages (from fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.5/site-packages (from jinja2==2.9.6->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /opt/conda/lib/python3.5/site-packages (from google-auth>=1.0.1->kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.5/site-packages (from google-auth>=1.0.1->kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.5/site-packages (from google-auth>=1.0.1->kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.5/site-packages (from google-auth>=1.0.1->kubernetes==3.0.0->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.5/site-packages (from ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.5/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython<6.0->fire==0.1.1->cli-pipeline==1.3.0)\r\n",
      "Installing collected packages: kubernetes, cli-pipeline\r\n",
      "  Found existing installation: kubernetes 3.0.0a1\r\n",
      "    Uninstalling kubernetes-3.0.0a1:\r\n",
      "      Successfully uninstalled kubernetes-3.0.0a1\r\n",
      "Successfully installed cli-pipeline-1.3.0 kubernetes-3.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install cli-pipeline==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Runs Directory Before Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: tensorflow\r\n",
      "model_name: mnist\r\n",
      "model_tag: test\r\n",
      "model_path: /root/notebooks/9 Extras/tensorflow/mnist\r\n",
      "deploy_server_url: http://admin.community.pipeline.ai\r\n",
      "timeout: 1200\r\n",
      "model_type: tensorflow\r\n",
      "model_name: mnist\r\n",
      "model_tag: test\r\n",
      "model_path: /root/notebooks/9 Extras/tensorflow/mnist\r\n",
      "tar_path: .\r\n",
      "filemode: w\r\n",
      "compression: gz\r\n",
      "\r\n",
      "Compressing model_path '/root/notebooks/9 Extras/tensorflow/mnist' into tar_path '/root/notebooks/9 Extras/tensorflow/mnist/tensorflow-mnist-test.tar.gz'.\r\n",
      "\r\n",
      "Deploying model tar.gz '/root/notebooks/9 Extras/tensorflow/mnist/tensorflow-mnist-test.tar.gz' to 'http://admin.community.pipeline.ai/api/v1/model/deploy/tensorflow/mnist/test'.\r\n",
      "Error while deploying model.\r\n",
      "Error: '('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))'\r\n",
      "\r\n",
      "\r\n",
      "Cleaning up temporary file tar '/root/notebooks/9 Extras/tensorflow/mnist/tensorflow-mnist-test.tar.gz'...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pipeline _cluster_deploy_tar --deploy-server-url=http://admin.community.pipeline.ai --model-type=tensorflow --model-name=mnist --model-tag=test --model-path=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
